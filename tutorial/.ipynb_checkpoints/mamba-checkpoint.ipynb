{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该文件作为参考案例，使用Pytorch框架，复现Mamba\n",
    "https://cloud.tencent.com/developer/article/2377967·"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型搭建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x23538522630>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from einops import rearrange\n",
    "\n",
    "import math\n",
    "import tqdm\n",
    "import os\n",
    "\n",
    "from urllib import request\n",
    "from zipfile import ZipFile\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置标志和超参数\n",
    "USE_MAMBA = 1\n",
    "DIFFERENT_H_STATES_RECURRENT_UPDATE_MECHANISM = 1\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义超参数和初始化\n",
    "d_model = 8\n",
    "state_size = 128 \n",
    "seq_len = 100\n",
    "batch_size = 256\n",
    "last_batch_size = 81 # only for the very last batch of the dataset\n",
    "current_batch_size = batch_size\n",
    "different_batch_size = False\n",
    "h_new = None\n",
    "temp_buffer = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S6模块\n",
    "S6模块是Mamba架构中的一个复杂组件，负责通过一系列**线性变换和离散化过程**处理输入序列。它在捕获序列的时间动态方面起着关键作用，这是序列建模任务(如语言建模)的一个关键方面。这里包括张量运算和自定义离散化方法来处理序列数据的复杂需求。\n",
    "S4模型定义四个参数$(\\Delta, A, B, C)$以及两个序列到序列的阶段：\n",
    "$$\n",
    "h'(t) = Ah(t)+Bx(t) (1a) \\quad h_t = \\hat{A} h_{t-1} + \\hat{B}x_t (2a) \\quad \\hat{K} = (C\\hat{B}, C\\hat{AB},...,C\\hat{A^k B},...) (3a)\\\\\n",
    "y(t) = Ch(t) (1b) \\quad y_t = Ch_t (2b) \\quad y = x * \\hat{K} (3b)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class S6(nn.Module):\n",
    "    def __init__(self, seq_len, d_model, state_size, device):\n",
    "        super(S6,self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(d_model, d_model, device=device)\n",
    "        self.fc2 = nn.Linear(d_model, state_size, device=device)\n",
    "        self.fc3 = nn.Linear(d_model, state_size, device=device)\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        self.d_model = d_model\n",
    "        self.state_size = state_size\n",
    "\n",
    "        self.A = nn.Parameter(F.normalize(torch.ones(d_model, state_size, device=device), p=2, dim=-1)) #这里为什么要进行归一化呢？\n",
    "        nn.init.xavier_uniform_(self.A)\n",
    "\n",
    "        self.B = torch.zeros(batch_size, self.seq_len, self.state_size, device=device)\n",
    "        self.C = torch.zeros(batch_size, self.seq_len, self.state_size, device=device)\n",
    "\n",
    "        self.delta = torch.zeros(batch_size, self.seq_len, self.d_model, device=device)\n",
    "        self.dA = torch.zeros(batch_size, self.seq_len, self.d_model, self.state_size, device=device)\n",
    "        self.dB = torch.zeros(batch_size, self.seq_len, self.d_model, self.state_size, device=device)\n",
    "\n",
    "        # h [B:batch_size, L:seq_len, D:d_model, S:state_size]\n",
    "        self.h = torch.zeros(batch_size, self.seq_len, self.d_model, self.state_size, device=device)\n",
    "        self.y = torch.zeros(batch_size, self.seq_len, self.d_model, device=device)\n",
    "\n",
    "    def discretization(self):\n",
    "        self.dB = torch.einsum(\"bld,bln->bldn\", self.delta, self.B)\n",
    "        self.dA = torch.exp(torch.einsum(\"bld,dn->bldn\", self.delta, self.A))\n",
    "        return self.dA, self.dB\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Algorithm 2 in MAMBA paper\n",
    "\n",
    "        self.B = self.fc2(x)\n",
    "        self.C = self.fc3(x)\n",
    "        self.delta = F.softplus(self.fc1(x))\n",
    "        self.discretization()\n",
    "\n",
    "        if DIFFERENT_H_STATES_RECURRENT_UPDATE_MECHANISM:\n",
    "            global current_batch_size\n",
    "            current_batch_size = x.shape[0]\n",
    "\n",
    "            if self.h.shape[0] != current_batch_size:\n",
    "                different_batch_size = True\n",
    "                h_new = torch.einsum('bldn,bldn->bldn', self.dA, self.h[:current_batch_size, ...]) + rearrange(x,'b l d -> b l d 1') * self.dB\n",
    "            else:\n",
    "                different_batch_size = False\n",
    "                h_new = torch.einsum('bldn,bldn->bldn', self.dA, self.h) + rearrange(x,'b l d -> b l d 1') * self.dB\n",
    "\n",
    "            # y [B, L, D]\n",
    "            self.y = torch.einsum('bln,bldn->bld', self.C, h_new)\n",
    "\n",
    "            global temp_buffer\n",
    "            temp_buffer = h_new.detach().clone() if not self.h.requires_grad else h_new.clone()\n",
    "\n",
    "            return self.y\n",
    "        \n",
    "        else:\n",
    "            # h [B, L, D, S]\n",
    "            h = torch.zeros(x.size(0), self.seq_len, self.d_model, self.state_size, device=device)\n",
    "            y = torch.zeros_like(x)\n",
    "\n",
    "            h = torch.einsum('bldn,bldn->bldn', self.dA, h) + rearrange(x, 'b l d -> b l d 1') * self.dB\n",
    "\n",
    "            # y [B, L, D]\n",
    "            y = torch.einsum('bln,bldn->bld', self.C, h)  \n",
    "\n",
    "            return y\n",
    "\n",
    "s6 = S6(seq_len=seq_len, d_model=d_model, state_size=state_size, device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSNorm\n",
    "RMSNorm 是一个自定义规范化层，这一层用于规范神经网络的激活，这可以帮助稳定和加快训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, d_model: int, eps: float = 1e-5, device: str = 'cpu'):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(d_model, device=device))\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = x * torch.rsqrt(x.pow(2).mean(dim=-1, keepdim=True) + self.eps) * self.weight\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MambaBlock(nn.Module):\n",
    "    def __init__(self, seq_len, d_model, state_size, device):\n",
    "        super(MambaBlock, self).__init__()\n",
    "        self.in_proj = nn.Linear(d_model, 2*d_model, device=device)\n",
    "        self.out_proj = nn.Linear(2*d_model, d_model, device=device)\n",
    "\n",
    "        # For residual skip connection\n",
    "        self.D = nn.Linear(d_model, 2*d_model, device=device)\n",
    "\n",
    "        # Set _no_weight_decay attribute on bias\n",
    "        self.out_proj.bias._no_weight_decay = True\n",
    "\n",
    "        # Initialize bias to a small constant value\n",
    "        nn.init.constant_(self.out_proj.bias, 1.0)\n",
    "\n",
    "        self.S6 = S6(seq_len, 2*d_model, state_size, device)\n",
    "\n",
    "        # Add 1D convolution with kernel size 3\n",
    "        self.conv = nn.Conv1d(seq_len, seq_len, kernel_size=3, padding=1, device=device)\n",
    "\n",
    "        # Add linear layer for conv output\n",
    "        self.conv_linear = nn.Linear(2*d_model, 2*d_model, device=device)\n",
    "\n",
    "        # rmsnorm\n",
    "        self.norm = RMSNorm(d_model, device=device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x_proj.shape = torch.Size([B, L, 2*D])\n",
    "        x_conv.shape = torch.Size([B, L, 2*D])\n",
    "        x_conv_act.shape = torch.Size([B, L, 2*D])\n",
    "        '''\n",
    "        # Refer to Fig.3 in MAMBA paper\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x_proj = self.in_proj(x)\n",
    "\n",
    "        # Add 1 D convolution with kernel size 3\n",
    "        x_conv = self.conv(x_proj)\n",
    "        \n",
    "        x_conv_act = F.silu(x_conv)\n",
    "\n",
    "        # Add linear layer for conv output\n",
    "        x_conv_out = self.conv_linear(x_conv_act)\n",
    "\n",
    "        x_ssm = self.S6(x_conv_out)\n",
    "        x_act = F.silu(x_ssm) # Switch activation can be implemented as x*sigmoid(x)\n",
    "\n",
    "        # residual skip connection with nonlinearity introduced by multiplication\n",
    "        x_residual = F.silu(self.D(x))\n",
    "\n",
    "        x_combined = x_act * x_residual\n",
    "\n",
    "        x_out = self.out_proj(x_combined)\n",
    "\n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mamba(nn.Module):\n",
    "    def __init__(self, seq_len, d_model, state_size, device):\n",
    "        super(Mamba, self).__init__()\n",
    "        self.mamba_block1 = MambaBlock(seq_len, d_model, state_size, device)\n",
    "        self.mamba_block2 = MambaBlock(seq_len, d_model, state_size, device)\n",
    "        self.mamba_block3 = MambaBlock(seq_len, d_model, state_size, device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mamba_block1(x)\n",
    "        x = self.mamba_block2(x)\n",
    "        x = self.mamba_block3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_output.shape = torch.Size([256, 100, 8])\n"
     ]
    }
   ],
   "source": [
    "# 用法\n",
    "x = torch.rand(batch_size, seq_len, d_model, device=device)\n",
    "\n",
    "# Create the Mamba model\n",
    "mamba = Mamba(seq_len, d_model, state_size, device)\n",
    "\n",
    "# rmsnorm\n",
    "norm = RMSNorm(d_model)\n",
    "x = norm(x)\n",
    "\n",
    "# Forward pass\n",
    "test_output = mamba(x)\n",
    "print(f\"test_output.shape = {test_output.shape}\") # should be [B, L, D]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据准备和训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Enwiki8Dataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data['input_ids'])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.data.items()}\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pad_sequences_3d用于将一批序列填充到统一的长度，确保批中的每个序列具有相同数量的元素(或时间步长)。这在许多机器学习任务中尤其重要，因为输入数据必须具有一致的形状。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for padding\n",
    "def pad_sequences_3d(sequences, max_len=None, pad_value=0):\n",
    "    # Assuming sequences is a tensor of shape (batch_size, seq_len, feature_size)\n",
    "\n",
    "    batch_size, seq_len, feature_size = sequences.size()\n",
    "    if max_len is None:\n",
    "        max_len = seq_len + 1\n",
    "\n",
    "    # Initialize padded_sequences with pad_value\n",
    "    padded_sequences = torch.full((batch_size, max_len, feature_size), fill_value=pad_value, dtype=sequences.dtype, device=sequences.device)\n",
    "    # Pad each sequence to the max_len\n",
    "    padded_sequences[:,:seq_len,:] = sequences\n",
    "\n",
    "    return padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, tokenizer, data_loader, optimizer, criterion, device, max_grad_norm=1.0, DEBUGGING_IS_ON=False):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_data = batch['input_ids'].clone().to(device)\n",
    "        attention_mask = batch['attention_mask'].clone().to(device)\n",
    "\n",
    "        target = input_data[:,1:]\n",
    "        input_data =  input_data[:,:-1]\n",
    "\n",
    "        # Pad all the sequences in the batch:\n",
    "        input_data = pad_sequences_3d(input_data, pad_value=tokenizer.pad_token_id)\n",
    "        target = pad_sequences_3d(target, max_len=input_data.size(1), pad_value=tokenizer.pad_token_id)\n",
    "\n",
    "        if USE_MAMBA:\n",
    "            output = model(input_data)\n",
    "            loss = criteerion(output, target)\n",
    "\n",
    "        loss.backward(retain_graph=True)\n",
    "\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'out_proj.bias' not in name:\n",
    "                # clip weights but not bias for out_proj\n",
    "                torch.nn.utils.clip_grad_norm_(param, max_grad_norm)\n",
    "\n",
    "        if DEBUGGING_IS_ON:\n",
    "            for name, parameter in model.named_parameters():\n",
    "                if parameter.grad is not None:\n",
    "                    print(f\"{name} gradient: {parameter.grad.data.norm(2)}\")\n",
    "                else:\n",
    "                    print(f\"{name} has no gradient\")\n",
    "\n",
    "        if USE_MAMBA and DIFFERENT_H_STATES_RECURRENT_UPDATE_MECHANISM:\n",
    "            model.S6.h[:current_batch_size, ...].copy_(temp_buffer)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估函数\n",
    "def evaluate(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_data = batch['input_ids'].clone().detach().to(device)\n",
    "            attention_mask = batch['attention_mask'].clone().detach().to(device)\n",
    "\n",
    "            target = input_data[:, 1:]\n",
    "            input_data = input_data[:,:-1]\n",
    "\n",
    "            # Pad all the sequences in the batch:\n",
    "            input_data = pad_sequences_3d(input_data, pad_value=tokenizer.pad_token_id)\n",
    "            target = pad_sequences_3d(target, max_len=input_data.size(1), pad_value=tokenizer.pad_token_id)\n",
    "\n",
    "            if USE_MAMBA:\n",
    "                output = model(input_data)\n",
    "                loss = criterion(output, target)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最后，calculate_perplexity用于评估语言模型(如Mamba)的性能。\n",
    "def calculate_perplexity(loss):\n",
    "    return math.exp(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_enwiki8_dataset函数用于下载和提取enwiki8数据集，该数据集通常用于对语言模型进行基准测试。\n",
    "def load_enwiki8_dataset():\n",
    "    print(f\"Download and extract enwiki8 data\")\n",
    "    url = \"http://mattmahoney.net/dc/enwik8.zip\"\n",
    "    request.urlretrieve(url, 'enwik8.zip')\n",
    "\n",
    "    with ZipFile('enwik8.zip') as f:\n",
    "        data = f.read('enwik8').decode('utf-8')\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode_dataset函数设计用于标记和编码数据集，为神经网络模型(如Mamba)处理数据集做准备。\n",
    "\n",
    "# Tokenizer and encode the dataset\n",
    "def encode_dataset(tokenizer, text_data):\n",
    "    def batch_encode(tokenizer, text_data, batch_size=1000):\n",
    "        # Tokenizer in batches\n",
    "        batched_input_ids = []\n",
    "        for i in range(0, len(text_data), batch_size):\n",
    "            batch = text_data[i:i+batch_size]\n",
    "            inputs = tokenizer(batch, add_special_tokens=True, truncation=True,\n",
    "                               padding='max_length', max_length=seq_len, return_tensors='pt')\n",
    "            batched_input_ids.append(inputs['input_ids'])\n",
    "        \n",
    "        return torch.cat(batched_input_ids)\n",
    "    \n",
    "    # Assuming enwiki8_data is a list of sentences\n",
    "    input_ids = batch_encode(tokenizer, enwiki8_data)\n",
    "\n",
    "    # vocab_size is the number of unique tokens in the tokenizer's vocabulary\n",
    "    global vocab_size\n",
    "    vocab_size = len(tokenizer.vocab) # Note that for some tokenizers, we might access the vocab directly\n",
    "    print(f'vocab_size = {vocab_size}')\n",
    "\n",
    "    # Create an embedding layer\n",
    "    # embedding_dim is the size of the embedding vectors (MAMBA model's D)\n",
    "    embedding_layer = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)\n",
    "\n",
    "    # Pass `input_ids` through the embedding layer\n",
    "    # This will change `input_ids` from shape [B, L] to [B, L, D]\n",
    "    def batch_embedding_calls(intput_ids, embedding_layer, batch_size=256):\n",
    "        # Check if input_ids is already a tensor, if not convert it\n",
    "        if not isinstance(intput_ids, torch.Tensor):\n",
    "            input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
    "\n",
    "        # Calculate the number of batches needed\n",
    "        num_batches = math.ceil(input_ids.size(0) / batch_size)\n",
    "\n",
    "        # List to hold the output embeddings\n",
    "        output_embeddings = []\n",
    "\n",
    "        # Process each batch\n",
    "        for i in range(num_batches):\n",
    "            # Calculate start and end indices for the current batch\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = start_idx + batch_size\n",
    "\n",
    "            # Get the batch\n",
    "            input_id_batch = input_ids[start_idx:end_idx]\n",
    "\n",
    "            # Call the embedding layer\n",
    "            with torch.no_grad(): # No need gradients for this operation\n",
    "                batch_embeddings = embedding_layer(input_id_batch)\n",
    "\n",
    "            # Append the batch embeddings to the list \n",
    "            output_embeddings.append(batch_embeddings)\n",
    "\n",
    "        # Concatenate the embeddings from each batch into a single tensor\n",
    "        all_embeddings = torch.cat(output_embeddings, dim=0)\n",
    "\n",
    "        return all_embeddings\n",
    "    \n",
    "    # `input_ids` is a list or tensor of the input IDs and `embedding_layer` is model's embedding layer\n",
    "    if USE_MAMBA:\n",
    "        # Set `batch_size` to a value that works for memory constraints\n",
    "        encoded_inputs = batch_embedding_calls(input_ids, embedding_layer, batch_size=1).float()\n",
    "\n",
    "    attention_mask = (input_ids != tokenizer.pad_token_id).type(input_ids.dtype)\n",
    "\n",
    "    return encoded_inputs, attention_mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading tokenizer_config.json: 100%|██████████| 48.0/48.0 [00:00<00:00, 15.5kB/s]\n",
      "d:\\Env\\anaconda3\\envs\\stgcn\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\lenovo\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading config.json: 100%|██████████| 570/570 [00:00<00:00, 406kB/s]\n",
      "Downloading vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 1.01MB/s]\n",
      "Downloading tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 2.05MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing raw data...\n",
      "Download and extract enwiki8 data\n",
      "vocab_size = 30522\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'input_ids' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_27564\\3242092036.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Tokenizing raw data...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0menwiki8_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_enwiki8_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mencoded_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencode_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menwiki8_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoded_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoded_inputs_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'finished tokenizing data'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_27564\\1390394054.py\u001b[0m in \u001b[0;36mencode_dataset\u001b[1;34m(tokenizer, text_data)\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mUSE_MAMBA\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;31m# Set `batch_size` to a value that works for memory constraints\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[0mencoded_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_embedding_calls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding_layer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[0mattention_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad_token_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_27564\\1390394054.py\u001b[0m in \u001b[0;36mbatch_embedding_calls\u001b[1;34m(intput_ids, embedding_layer, batch_size)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;31m# Calculate the number of batches needed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0mnum_batches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;31m# List to hold the output embeddings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'input_ids' referenced before assignment"
     ]
    }
   ],
   "source": [
    "# Load a pretrained tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Assuming encoded_inputs is a preprocessed tensor of shape [num_samples, seq_len, d_model]\n",
    "encoded_inputs_file = 'encoded_inputs_mamba.pt'\n",
    "\n",
    "if os.path.exists(encoded_inputs_file):\n",
    "    print(\"Loading pre-tokenized data...\")\n",
    "    encoded_inputs = torch.load(encoded_inputs_file)\n",
    "\n",
    "else:\n",
    "    print(\"Tokenizing raw data...\")\n",
    "    enwiki8_data = load_enwiki8_dataset()\n",
    "    encoded_inputs, attention_mask = encode_dataset(tokenizer, enwiki8_data)\n",
    "    torch.save(encoded_inputs, encoded_inputs_file)\n",
    "    print(f'finished tokenizing data')\n",
    "\n",
    "# Combine into a single dictionary\n",
    "data = {\n",
    "    'input_ids': encoded_inputs,\n",
    "   'attention_mask': attention_mask\n",
    "}\n",
    "\n",
    "# Split the data into train and validation sets\n",
    "total_size = len(data['input_ids'])\n",
    "train_size = int(total_size * 0.8)\n",
    "\n",
    "train_data = {key: val[:train_size] for key, val in data.items()}\n",
    "val_data = {key: val[train_size:] for key, val in data.items()}\n",
    "\n",
    "train_dataset = Enwiki8Dataset(train_data)\n",
    "val_dataset = Enwiki8Dataset(val_data)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the model\n",
    "model = Mamba(seq_len, d_model, state_size, device).to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-6)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 25 # Number of epochs to train for\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)): # loop over the dataset multiple times\n",
    "    train_loss = train(model, tokenizer, train_loader, optimizer, criterion, device, max_grad_norm=10, DEBUGGING_IS_ON=False)\n",
    "    val_loss = evaluate(model, val_loader, criterion, device)\n",
    "    val_perplexity = calculate_perplexity(val_loss)\n",
    "    print(f'Epoch: {epoch+1}, Training Loss: {train_loss:.4f}, Validation Loss:{val_loss:.4f}, Validation Perplexity:{val_perplexity:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
